---
title: "MovieLens"
author: "Daoud Youssef"
date: "<small>`r Sys.Date()`</small>"
bibliography: movielens.bib
output:
  pdf_document:
    toc: yes
    toc_depth: '3'
  html_document:
    toc: yes
    toc_depth: 3
    number_sections: yes
    toc_float:
      collapsed: yes
    code_folding: hide
    theme: spacelab
---

```{r echo=FALSE}
library(formatR) 
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)
```

# Introduction 

This project is part of the Harvard: PH125.9 course Data science: Capstone course. It consists of building  a recommender system to predict the rating of a movie given by a user. 

Originally, Netflix awarded a $\$1$M Grand Prize to improve the accuracy of rating predictions ^[ https://www.netflixprize.com] by 10%.

The aim of this project is to build an algorithm to predict the rating of a movie.
The evaluation of the validity of our model is based on the Residual Mean Squared Error RMSE. The target is to reach a value below $0.86499$. This error is interpreted as movie rating and thus an error of value $1$ means an error of $1$ star in predicting. [@rafael]

This project is organized as follows:  In section 2, we present the dataset used and a detailed explanation on all of the models. In section 3, we explore the ```Edx``` dataset and its statistical properties. In addition we present the algorithms of the models used in this project. In Section 4, we discuss the results and we conclude in section 5. 

# Methodology
## Dataset
First, we load libraries and data. This code is provided by the ```Edx``` team.^[https://courses.edx.org/courses/course-v1:HarvardX+PH125.9x+1T2020/courseware/dd9a048b16ca477a8f0aaf1d888f0734/e8800e37aa444297a3a2f35bf84ce452/?activate_block_id=block-v1%3AHarvardX%2BPH125.9x%2B1T2020%2Btype%40sequential%2Bblock%40e8800e37aa444297a3a2f35bf84ce452]. This code stopped running and alernatevily, I used downloaded packages for data.

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(caret)
library(data.table)
library(lubridate)
```

```{r eval=FALSE, warning=FALSE}
if (!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if (!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if (!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))), col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId], title = as.character(title), genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")
```


```{r echo=FALSE,eval=FALSE}
set.seed(1, sample.kind = "Rounding")
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index, ]
temp <- movielens[test_index, ]
# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>%
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)
```

```{r}
## Code provided above from edx stopped working so I used a file saved before
edx <- readRDS("edx.rds")
validation <- readRDS("validation.rds")
```


There are 2 sets used in this project :

1. **Edx** set which is used to analyze the predictors and build our model. This data set has `r nrow(edx)` rows and `r ncol(edx)` columns. It is is partitioned into two sets ```train_edx``` and ```test_edx```. The ```train_set``` represents $90\%$ of the ```edx``` data.
2. **Validation** set is used to evaluate and validate our final model. This dataset has `r nrow(validation)` and `r ncol(validation)` columns. As per the project guidelines, the validation set will be $10\%$ of the MovieLens data.

We will use linear regression model to predict the rating of the movie based on our variables.
The goal is to examine how strong is the relationship between the outcome and predictors ^[search for it]. 

For this project, we build $3$ models. First model, baseline model, we used the mean of ratings as predictor. Second model, we add the movie effect as a new parameter. Third model, the user effect was added to the previous model.

To optimize our model, we regularized it by introducing a new parameter. The goal is always to minimize the sum of the squared residuals and give our model more stability.
 
Validation of our model is based on the value of Root Mean Square Error **RMSE**. The **RMSE**  loss function is calculated as follows: 
\begin{equation} RMSE=\sqrt{\frac{1}{N}\sum_{u,i}\left( y_{u,i}-\hat{y}_{u,i}\right)^2} \end{equation}
with 

* $y_{u,i}$ the actual ratings,
* $\hat{y}_{u,i}$ the predicted ratings.
* $N$ number of possible combinations between user $u$ and movie $i$.


The code that compute the RMSE is : 

```{r eval=FALSE}
RMSE <- function(true_ratings, predicted_ratings) {
  sqrt(mean((true_ratings - predicted_ratings)^2))
}
```

Finally, we use the validation set to test our model.

# Data analysis and model preparation

## Data exploration and visualization

First, the ```edx``` dataset contains no missing values.  

```{r}
anyNA(edx)
```

The ```edx``` dataset contains 6 variables.  ```UserId``` and ```movieId``` are our predictors  and ```rating``` is the outcome. 

```{r}
str(edx)
```

```{r}
head(edx)
```
 

```{r}
summary(edx)
```

From the code below, we can conclude that there is no zero rating for any movie,

```{r}
sum(edx$rating == 0)
```


```{r}
user <- edx %>% summarize(n_users = n_distinct(userId), n_movies = n_distinct(movieId))
knitr::kable(user, col.names = c("Number of users", "Number of Movies"))
```

`Edx` data set has `r user$n_users` different users and `r user$n_movies` different movies. Therefore, Some users are rating more than one movie.

```{r}
rate_distribution <- edx %>%
  group_by(rating) %>%
  summarize(n = n())
rate_distribution %>% knitr::kable(col.names = c("Rating", "Number of rates"))
edx %>%
  group_by(rating) %>%
  ggplot(aes(x = rating)) +
  geom_histogram(bins = 30) +
  scale_y_continuous(labels = function(x) format(x, big.mark = " ", scientific = FALSE))
```

From the rating distribution, we notice the following:

1. Half star ratings are less rated than whole star ratings.
1. The 4 star rating is the most applied among other ratings.


```{r number of movie rating}
edx %>%
  group_by(movieId) %>%
  summarize(number_movies = n()) %>%
  ggplot(aes(number_movies)) +
  geom_histogram(bins = 30, color = "grey") +
  scale_x_log10() +
  ggtitle("Distribution of movies") +
  xlab("Number of movies") +
  ylab("Number of ratings")
```


```{r separate genres}
genre <- edx %>%
  separate_rows(genres, sep = "\\|") %>%
  group_by(genres) %>%
  summarize(count = n()) %>%
  arrange(desc(count))
genre %>% ggplot(aes(x = reorder(genres, count), y = count)) +
  geom_bar(stat = "identity", fill = "lightgrey") +
  labs(x = "", y = "Number of ratings") +
  coord_flip() +
  labs(title = "Number of ratings by movie genre")

genre_rating <- edx %>%
  separate_rows(genres, sep = "\\|") %>%
  group_by(genres) %>%
  summarize(n = mean(rating)) %>%
  arrange(desc(n))

genre_rating %>% ggplot(aes(x = n, y = reorder(genres, n))) +
  geom_bar(stat = "identity", fill = "lightgrey") +
  labs(x = " Average ratings per Genres", y = "Genres") +
  labs(title = "Average of ratings by movie genre")
```


From the chart above, we conclude that Drama is the top genre rated according to users, followed by Comedy and action. We note that 7 movies are not been categorized by any genre. Film-noir genre has a high rating level but less number of ratings. Drama is more rated than others but as average rating is not high. 

The 10 most rated movies are shown below. 5 of these movies are categorized as Drama and 4 as Action. 

```{r top 10 movies}
top_10 <- edx %>%
  group_by(title, genres) %>%
  summarize(n = n()) %>%
  arrange(desc(n))

top_10[1:10, ] %>% ggplot(aes(x = reorder(title, n), y = n)) +
  geom_bar(stat = "identity", fill = "lightgrey") +
  labs(x = "", y = "Number of ratings") +
  coord_flip() +
  labs(title = "Top 10 movies")
```


```{r top 10 movies with their rating}
ratebygenre <- edx %>%
  group_by(genres) %>%
  summarize(n = n(), mean_rate = mean(rating)) %>%
  arrange(desc(n))
ratebygenre[1:10, ] %>% 
  knitr::kable(col.names = c("Genre", "Number of ratings", "Average of ratings"))
```

From this table, we can conclude that the 10 movies with the highest number of ratings do not have the highest average in rating.

```{r}
edx$timestamp <- as_datetime(as.POSIXct(edx$timestamp, origin = "1970-01-01"))
edx_d <- edx %>% 
  mutate(days = weekdays(edx$timestamp), monthss = months(edx$timestamp), 
         years = year(edx$timestamp))
edx_d$days <- factor(edx_d$days, 
            levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")) ## ordering days
edx_d$monthss <- factor(edx_d$monthss, levels = c("January", "February", "March", "April", "May", "June", "July", "August", "September", "October", "November", "December")) ## ordering months
edx_d %>%
  group_by(monthss) %>%
  summarize(n = n()) %>%
  arrange(desc(n)) %>%
  knitr::kable(col.names = c("Month", "Number of ratings")) ## rating distribution based on months
edx_d %>%
  group_by(days) %>%
  summarize(n = n()) %>%
  arrange(desc(n)) %>%
  knitr::kable(col.names = c("Days", "Number of ratings")) ## rating
edx_d %>%
  group_by(years) %>%
  summarize(n = n()) %>%
  arrange(desc(n)) %>%
  knitr::kable(col.names = c("Years", "Number of ratings"))
```

Tuesday is the day where most users rate. November is the month where the highest number of ratings is attained and $2000$ is the year where the most number of ratings has been made.

```{r comparison based on weekdays}
edx_d %>%
  group_by(days, rating) %>%
  summarize(n = n()) %>%
  ggplot(aes(days, n)) +
  geom_point(stat = "identity", aes(color = rating, size = rating)) +
  scale_color_gradient(low = "yellow", high = "blue") +
  labs(x = "Weekday", y = "Number of ratings", title = "Weekday rating comparison")+ theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

As shown by the graph above, weekdays do not seem to have a strong influence on the rating level of the movies.

```{r comparison based on months}
edx_d %>%
  group_by(monthss, rating) %>%
  summarize(n = n()) %>%
  ggplot(aes(monthss, n, fill = monthss)) +
  geom_bar(stat = "identity") +
  labs(x = "Months", y = "Number of ratings",
       title = "Months rating comparison")+ 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
November and December have the highest number of rating respectively. 


```{r weekday and month rating comparison}
edx_d %>%
  group_by(days, monthss) %>%
  summarize(n = n()) %>%
  ggplot(aes(monthss, n)) +
  geom_bar(stat = "identity", aes(fill = days), position = "dodge") +
  labs(x = "Month", y = "Number of ratings", 
       title = "Weekday and month rating comparison")+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r}
edx_d %>%
  group_by(days, monthss, rating) %>%
  summarize(n = n()) %>%
  ggplot(aes(days, n)) +
  geom_jitter(aes(color = rating, size = rating), alpha = 0.55) +
  scale_color_gradient(low = "yellow", high = "blue") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  facet_wrap(vars(monthss))
```

## Modeling

As we have mentioned above, we excluded $10\%$ of ``Edx`` dataset for testing and we kept $90\%$ for training. 

```{r data partitioning, message=FALSE, warning=FALSE}
# edx data set will splited into training (90%) and test set.
set.seed(1, sample.kind = "Rounding")
edx_index <- createDataPartition(y = edx$rating, times = 1, p = 0.1, list = FALSE)
train_edx <- edx[-edx_index, ]
temp <- edx[edx_index, ]
## make sure userID and movie Id in test set and in train set
test_edx <- temp %>%
  semi_join(train_edx, by = "movieId") %>%
  semi_join(train_edx, by = "userId")
## add rows from test set back intor train set
removed <- anti_join(temp, test_edx)
train_edx <- rbind(train_edx, removed)
rm(edx_index, temp, removed)
## clean data
train_edx <- train_edx %>% select(userId, movieId, rating, title)
test_edx <- test_edx %>% select(userId, movieId, rating, title)
```

### Model 1: Average movie rating

The first model is the simplest one. We will be predicting using the average of movie ratings.
A rating is approximated as :
\begin{equation} 
Y_{u,i}=\mu+\epsilon_{u,i} 
\end{equation} 

* $\mu$ is the "true" rating of all movies.
* $\epsilon_{u,i}$ is the independent errors sampled from the same distribution centered at $0$

```{r baseline}
mu_hat <- mean(train_edx$rating)
baseline_RMSE <- RMSE(test_edx$rating, mu_hat)
RMSE_results <- data.frame(Method = "Average rating movie model", RMSE = baseline_RMSE)
```


### Model 2: Movie effect model

In the second model, we add another variable: movie effect. We know that some movies have higher rating than others. We add another term  $b_i$ to represent average ranking for movie $i$. 
our model becomes:
\begin{equation}
Y_{u,i}=\mu+b_i+\epsilon_{u,i} 
\end{equation}

```{r model 2}
### Model 2 : adding movie effect  b_i
movie_avgs <- train_edx %>%
  group_by(movieId) %>%
  summarize(b_i = mean(rating - mu_hat))
#head(movie_avgs)
# predict the rating with b_v
y_hat_movie_avgs <- mu_hat + test_edx %>%
  left_join(movie_avgs, by = "movieId") %>%
  pull(b_i)
rmse_b_i <- RMSE(test_edx$rating, y_hat_movie_avgs)
## add the results to RMSE results
RMSE_results <- rbind(RMSE_results, 
            data.frame(Method = "Average rating movie + b_i ", RMSE = rmse_b_i)) 
```

### Model 3: Movie and user effect model

Every user rates the movie based on his likes or dislikes and affect the overall rating of the movie. Adding the user effect $b_u$ will improve our model. Therefore, a lower RMSE can be achieved. Our model is as follows: 
\begin{equation}
Y_{u,i}=\mu+b_i+b_u+\epsilon_{u,i} 
\end{equation}

```{r model 3}
movie_users <- train_edx %>%
  left_join(movie_avgs, by = "movieId") %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu_hat - b_i))
#head(movie_users)
# predict the rating with b_i
y_hat_model <- test_edx %>%
  left_join(movie_avgs, by = "movieId") %>%
  left_join(movie_users, by = "userId") %>%
  mutate(pred = mu_hat + b_i + b_u) %>%
  pull(pred)
rmse_b_i_b_u <- RMSE(test_edx$rating, y_hat_model)
## add the results to RMSE results
RMSE_results <- rbind(RMSE_results,
          data.frame(Method = "Average rating movie + b_i +b_u", RMSE = rmse_b_i_b_u))
```

## Model 4: Regularized movie and user effect model.

In our algorithms, we are using rating average and RMSE. To prevent overfitting and the total variability of the effect sizes, we use regularization. It consists of adding a new parameter in order to minimize the loss function. We included $\lambda$ and tuned it empirically to get the lowest RMSE. We used cross validation method to choose it. 

#### Regularized movie effect model

The new model and $b_i$ that minimizes the initial model is calculated as follows:

\begin{equation}
\hat{b}_i(\lambda)=\frac{1}{\lambda+N}\sum_{u=1}^{n_i}(Y_{u,i}-\hat{\mu})
\end{equation}

with 

* $N$ is the number of ratings for each movie. 

```{r}
### regularized movie  effect model
lambdas_e <- seq(0, 10, 0.25)
rmses_e <- sapply(lambdas_e, function(l) {
  mu <- mean(train_edx$rating)
  b_i <- train_edx %>%
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu) / (n() + l))
  predicted_ratings <- test_edx %>%
    left_join(b_i, by = "movieId") %>%
    mutate(pred = mu + b_i) %>%
    pull(pred)
  return(RMSE(predicted_ratings, test_edx$rating))
})
#qplot(lambdas_e, rmses_e)
lambda_e <- lambdas_e[which.min(rmses_e)]
RMSE_results <- rbind(
  RMSE_results,
  data.frame(Method = "Regularized Movie Effect Model", RMSE = min(rmses_e))
)

```

#### Regularized movie and user effect model

The new model and $b_i$ that minimize the initial model is calculated as follows:

\begin{equation}
\hat{b}_i(\lambda)=\frac{1}{\lambda+N}\sum_{u=1}^{n_i}(Y_{u,i}-b_i-\hat{\mu})
\end{equation}

with 

* $N$ is the number of ratings for each movie. 


```{r}
lambdas_u <- seq(0, 10, 0.25)
rmses_u <- sapply(lambdas_u, function(l) {
  mu <- mean(train_edx$rating)
  b_i <- train_edx %>%
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu) / (n() + l))
  b_u <- train_edx %>%
    left_join(b_i, by = "movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu) / (n() + l))
  predicted_ratings <-
    test_edx %>%
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mu + b_i + b_u) %>%
    pull(pred)
  return(RMSE(predicted_ratings, test_edx$rating))
})
#qplot(lambdas_u, rmses_u)
lambda_u <- lambdas_u[which.min(rmses_u)]
reg_b_i_b_u<-min(rmses_u)
RMSE_results <- rbind(
  RMSE_results,
  data.frame(Method = "Regularized Movie + User Effect Model", RMSE = min(rmses_u))
)

```

## Validating the model 

We validate our model using the code below. Training set is ```Edx``` dataset and testing set is the ```validation``` dataset. 

```{r}
mu_edx <- mean(edx$rating)
bi_edx <- edx %>%
  group_by(movieId) %>%
  summarize(b_i = sum(rating - mu_edx) / (n() + lambda_u))

bu_edx <- edx %>%
  left_join(bi_edx, by = "movieId") %>%
  group_by(userId) %>%
  summarize(b_u = sum(rating - b_i - mu_edx) / (n() + lambda_u))
predicted_ratings <- validation %>%
  left_join(bi_edx, by = "movieId") %>%
  left_join(bu_edx, by = "userId") %>%
  mutate(pred = mu_edx + b_i + b_u) %>%
  pull(pred)
val_pred <- RMSE(validation$rating, predicted_ratings)


RMSE_results <- rbind(RMSE_results, 
                      
  data.frame(Method = "Validation of edx data set using Movie and User Effect Model",
             RMSE = val_pred))
```


# Results
## Parameter $\lambda$
For the movie effect model, the optimal $\lambda$ is : 

```{r}
qplot(lambdas_e, rmses_e)
lambda_e <- lambdas_e[which.min(rmses_e)]
lambda_e
```

For the movie and user effect, the optimal $\lambda$ is : 

```{r}
qplot(lambdas_u, rmses_u)
lambda_u <- lambdas_u[which.min(rmses_u)]
lambda_u
```



## RMSE Results
Results are shown below: 

```{r}
knitr::kable(RMSE_results)
```

The table above shows that the best *RMSE* achieved in the regularized movie and user effect model increased by `r round(abs((reg_b_i_b_u-val_pred))/reg_b_i_b_u*100,2)`$\%$ when validating our model.

# Conclusion and discussion

This project started by exploring and visualizing the MovieLens data. We built our models starting from the baseline till a more complicated model. The *RMSE* has been reached.
In a film classroom how long a movie lasts in cinemas is an indicator to its success, as well as if it survives and remains interesting with time. It’s interesting to include temporal variability as predictor for two reasons. First, a user rating may change over time. Second, popularity of movies changes over time as well. A function of time may achieve more consistency in predicting. (example)A low-resolution movie can be rated lower now due to the existing of full HD or 4K movies despite the genre and quality of the movie. Including the duration of a movie in the dataset can also play an important parameter in determining its rating..

# References 

